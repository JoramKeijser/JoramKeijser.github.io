---
image: /images/lyapunov/double_T400_I30_elev20_rotate2x_dpi72.gif 
title: "Safe machine learning with neural Lyapunov functions"
---

## Safe machine learning with neural Lyapunov functions

<p align="center">
<img src="/images/lyapunov/double_T400_I30_elev20_rotate2x_dpi72_up.gif"  style="background:none; border:none; box-shadow:none;">
</p>
<span class="caption" STYLE="font-size:85%"> The pendulum loses energy until it stops in an equilbrium state (hanging down) with energy 0. 
 Since energy doesn't increase, the equilibrium position must be stable. Similar arguments can be used to certify the stability of neural network-controlled systems. This involves so-called Lyapunov functions that, like energy, are non-negative and decreasing. </span>

Over the past decade, deep learning has enabled incredible performance in areas ranging from image recognition to protein folding and game play. But for all their success, deep networks can be surprisingly brittle—a major challenge for their safe use in the real world. 

One solution is to borrow inspiration from machine learning-adjacent fields that emphasise safety. A prime example is control theory, a field with similar goals but a longer track record of real-world applications, that includes intensive care and fighter jets. But control theory is also more limited in scope, since it involves less learning and therefore more domain knowledge. 

Combining deep learning with control theory could therefore create the best of both worlds: algorithms that are simultaneously expressive and safe. This blog post is about such a combination, known as Lyapunov functions. Lyapunov functions are a key control theoretic tool for proving a system’s stability (a form of safety), yet they are hard to use. In his famous [textbook](https://www.stevenstrogatz.com/books/nonlinear-dynamics-and-chaos-with-applications-to-physics-biology-chemistry-and-engineering), Steven Strogatz writes that:

> "Unfortunately, there is no systematic way to construct Lyapunov functions. 
Divine inspiration is usually required (...)" 

But Strogatz wrote this over 20 years ago, and since then systematic ways to construct Lyapunov functions have in fact been developed—relying not on divine inspiration but on optimization. Most recently, these methods have been combined with deep neural networks. Here, we will compare these “neural Lyapunov functions”. 

More coming soon. 

