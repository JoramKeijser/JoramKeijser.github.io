
# Low-dimensional manifolds in neuroscience and evolution

The brain contains billions of neurons, so you might think we need a billion numbers to describe the brain's activity. But in practice, we don’t need a billion numbers -- just a handful often goes a long way. These numbers are said to define a low-dimensional *manifold* in neural space, and their existence implies that the brain typically visits only a small fraction of all its potential states. There is an analogous observation in evolutionary biology: The number of existing organisms is only a tiny fraction of all potential organisms. For example, there are no (and have never been) unicorns or centaurs, even though they are obviously conceivable to the human mind. In this blog pots, we will examine these analogous neural and evolutionary/developmental phenomena, and see what lessons can be draw from their similarity. 

> **tl;dr** Low-dimensional patterns in neural space and organismal space are both caused by functional reasons, but also by constraints. This analogy suggests several insights for neuroscience, for example that disambiguating function and constraint requires a causal test. Such causal tests have, in fact, been done in several clever experiments that used brain computer interfaces and artificial selection. 

<img src="/images/1analogy.jpg" style="background:none; border:none; box-shadow:none;">
<span class="caption"> (a) Population activity lies on a low-dimensional manifold in neural space. Each dimension corresponds to the activity of one neuron. (b) Variation of wing structure across a population of *Drosophila* lies on a low-dimensional manifold in phenotype space. Each dimension corresponds to the intensity of one pixel from an image of the wing. (a) From [Gallego et al. 2017](https://doi.org/10.1016/j.neuron.2017.05.025), (b) from [Alba et al. 2021](https://doi.org/10.7554/eLife.66750).</span>

### The finding: low-dimensional manifolds
Over a decade of large-scale neural recordings has revealed that the activity of a large number of neurons can often be captured by a much smaller number of key dimensions. In auditory cortex, for example, neural population activity rotates in a low-dimensional space that depends on the sound stimulus, which sets the population's initial state ([Bondanelli et al.](https://doi.org/10.7554/eLife.53151)). The initial state can also be used to incorporate prior expectations into neural dynamics, integrating them with sensory evidence in a Bayes-optimal way ([Sohn et al.](https://doi.org/10.1016/j.neuron.2019.06.012)). Although in both of these cases, neural activity is not fully explained by two or three dimensions, these dimensions do relate to behavior in an interpretable way. But human interpretability is not one of the brain’s design principles, raising the question why neural activity should be low-dimensional. 

<img src="/images/2neuralexamples.jpg" style="background:none; border:none; box-shadow:none;">
<span class="caption"> (a) Two-dimensional rotations in auditory cortex after a sound is turned off. The two colours correspond to different sounds that elicited activity in orthogonal planes. (b) Activity in frontal cortex during time estimation. The activity is warped depending on subjects' prior expectation of the time interval (shown in blue and red), and the actual duration of the time interval (shown in dark vs. light). (a) From [Bondanelli et al. 2021](https://doi.org/10.7554/eLife.53151),  (b) from [Sohn et al. 2019](https://doi.org/10.1016/j.neuron.2019.06.012) </span>


It's much easier to observe and compare a lot of animals than to record from a lot of neurons, and that's probably why comparative biologists have long known that phenotypes are also low-dimensional. For example, most mammals have seven neck vertebrae, even though the long-necked giraffe might benefit from more, and the neckless whale could do with less. In phenotype space, therefore, only the subspace of mammals with seven neck vertebrae is densely occupied. Body forms within species can also be surprisingly simple: Fruit fly wings are well-approximated by a single dimension (see figure at the top from [Alba et al.](https://doi.org/10.7554/eLife.66750))! What causes such a sparse occupation of neural an phenotype spaces?

### Explanation 1: function 
The first explanation for low-dimensional patterns in neural and phenotype space is functional. Low-dimensional neural activity can result from low-dimensional neural computation: If a brain area is engaged in a simple computation (say, integrating a noisy variable over time), it’s activity will also be simple. This intuition was formalised by [Gao et al.](https://doi.org/10.1101/214262), who showed that besides the number of task variables, the smoothness of neural activity also matters. If we record for a limited amount of time, and neural activity cannot behave erratically, the total number of brain states is necessarily limited. 

<img src="/images/3theory.jpg" style="background:none; border:none; box-shadow:none;">
<span class="caption"> If a subject is doing a relatively simple task like reaching, its neural activity varies smoothly in time, and we don't record for very long - activity will be low-dimensional. From [Gao et al. 2017](https://doi.org/10.1101/214262)  </span>

Just like the brain learns to generate activity that’s suitable for the current task, natural selection favours variants that are fit to the current environment. If, therefore, only a few variants are the fittest, they are likely to become dominant. In a famous example, black-coloured peppered moths used to be rare because they were easily spotted by predators when sitting on a light tree. During the industrial revolution, however, pollution caused trees to be darker, making light-coloured moths the easier ones to spot! Natural selection therefore favoured black moths over their lighter brethren, making them the dominant variant. 
This pattern reversed once pollution decreased. At each point in time, selection therefore favours a limited set of phenotypes, making alternatives rare or non-existent. Both neural activity and phenotypes can therefore be limited to a small set of “optimal” patterns.

<img src="/images/4moths.jpg" style="background:none; border:none; box-shadow:none;">
<span class="caption"> (a) Peppered moths can have a light or a dark (melanic) colour. Air pollution made trees darker, and therefore decreased the visibility of melanic moths. (b) The pattern reversed when air pollution decreased. (a) from [Wikipedia](https://commons.wikimedia.org/wiki/File:Lichte_en_zwarte_versie_berkenspanner.jpg), (b) from [Hedrick 2006](https://www.annualreviews.org/doi/abs/10.1146/annurev.ecolsys.37.091305.110132). </span>

### Explanation 2: constraints 
But sometimes the brain cannot produce a certain pattern of neural activity, even when it would be best to do so. Similarly, a certain phenotype might fail to evolve, even when it would increase a species’ fitness or save it from extinction. In this case, the lack of variability must be due to the presence of constraints. For neural activity, such a constraint can be caused by neural connectivity. If, for example, two excitatory neurons are reciprocally connected, this might exclude activity patterns in which only one of them is active. Of course, synaptic plasticity could weaken their connectivity, but this takes time. In the case of evolution, a constraint might be due to a single gene that affects multiple traits, such that changing one trait but not the other is genetically impossible. A similar constraint can also arise when two traits are coded for by different genes, but these genes are inherited together (for example because they’re located nearby on the same chromosome). Over time, these constraints could be alleviated by mutations, but just like synaptic plasticity, this takes time.

In sum, there are analogous reasons for the low-dimensional patterns in neural space and animal space. This points to fundamental similarities between their origins. Neural activity is shaped by learning, and animal forms are shaped by natural selection. Both learning and natural selection tend to increase some kind of performance or fitness measure. But neither does so from scratch, and this limits how much they can change things in the short run.  

### Testing neural constraints
Given that there are multiple possible causes of low-d patterns, we would like to know which one is at play in any particular case. Neuroscientists and evolutionary biologists have both come up with elegant experiments to test exactly this. In neuroscience, [Sadtler et al.](https://doi.org/10.1038/nature13665) used a brain computer interface to test the neural constraints on learning. A brain computer interfat (BCI) maps neural activity to an actuator, in this case a cursor on a computer screen. This allows great experimental control, since (1) it's only the neurons you record that directly influence behavior (no subsampling), and (2) these neurons influence behavior via the so-called BCI mapping, which is known and even under your control. 

Sadtler et al. used the experimental control provided by a brain computer interface in in a very clever way. They started out by identifying a low-dimensional “intuitive manifold” of activity patterns that subjects could easily generate. The manifold was simply a principal subspace of subjects' neural activity while they passively watched the cursor moving on the screen[^1]. Next, the authors challenged their subjects to generate new patterns by perturbing the BCI mapping. Crucially, these patterns came in two types: one type was inside the intuitive manifold, the other wasn’t. The inside manifold perturbation  projected neural activity onto the intuitive manifold as before, but then permuted the manifold dimensions before feeding it to the BCI readout. Adapting to the inside-manifold perturbation therefore required subjects to change the neural activity for moving the cursor in a particular direction, but this neural activity was still part of their previous "repertoire". The outside manifold perturbation, on the other hand, projected activity onto a subspace different from the intuitive manifold. Controlling the cursor after this change to the BCI mapping therefore required subjects to generate patterns outside of the intuitive manifold. The key finding froom the experiment: Subjects could quickly adapt to inside manifold perturbations, but not outside manifold perturbations. This failure suggests that the low-dimensionality of the intuitive manifold was not due to functional requirements, but rather to constraints. Later work from the same labs showed that neural activity is not very flexible within the manifold [Golub et al.](https://doi.org/10.1038/s41593-018-0095-3), but that new activity patterns can be learned when subjects are given enough time [Oby et al.](https://doi.org/10.1073/pnas.1820296116).

<img src="/images/5neuralconstraints.jpg" style="background:none; border:none; box-shadow:none;">
<span class="caption"> Neural constraints on learning [Sadtler et al. 2014](https://doi.org/10.1038/nature13665). Neural activity is usually confined to an intuitive manifold. Changes to the BCI readout within this manifold (red) are easy to learn, changes outside
the manifold (blue) are hard to learn. </span>

### Testing evolutionary constraints
Evolutionary-developmental biologists have also tested the causes of low-d phenotypes. Brakefeld et al. ([Beldade et al.](https://doi.org/10.1038/416844a), [Allen et al. 2008](https://doi.org/10.1186/1471-2148-8-94)) asked why the two eyespots on the wings of butterflies typically have a similar colour and size. Do similar eyespots convey a fitness advantage (e.g., asymmetric butterflies are picked out by predators), or are they just easier to generate by development? Brakefeld et al. put this to the test using artificial selection experiments. In one experiment, they selected butterfly strains for one of the four possible combinations of spot size: the two correlated combinations (small/small and large/large) and the two anti-correlated combinations (small/large and large/small). In another experiment, they selected butterflies based on the eyespot *colour*. Interestingly, the two experiments showed opposite outcomes: It was possible to evolve anti-correlated sizes of spots, but not to evolve anti-correlated colours. This suggests that similarly sized spots are the result of selection (different sizes are possible, but apparently not selected), but similarly coloured spots colours are the result of constraints (different colours are not possible).

<img src="/images/6devoconstraints.jpg" style="background:none; border:none; box-shadow:none;">
<span class="caption"> Developmental constraints on learning [Allen et al. 2008](https://doi.org/10.1186/1471-2148-8-94). The eye spots on butterfly wings are correlated in both size and colour. Artificial selection cannot break the correlation between the colours (left), but it can break the correlation between the sizes (right).</span>

The difference in flexibility suggests that the colour and size of eyespots are determined by different developmental mechanisms. This is indeed the case. 
During development, eyespot size is regulated by signalling from clusters of organiser cells. Each (potential) eyespot has its own organiser, and its mature size will therefore be relatively independent of neighbouring eyespots. Eyespot colour, on the other hand, develops in response to the concentration gradient of a diffusive signal. This concentration therefore cannot be regulated independently for each eyespot, and neither can the response to a given concentration. Some generation did show a lower correlation across eyespot sizes, but this was not inherited by their offspring, suggesting that the lower correlation was due to variability in individual development instead of genetics [Allen et al. 2008](https://doi.org/10.1186/1471-2148-8-94). The different causes of phenotypic correlations would have been difficult to predict from observational data alone, since the correlation across eyespot color and size are similar in wild type butterflies. These experiments therefore not only show that either selective pressure or genetic constraints might can a role, they also show that disambiguating the two requires causal experiments. 


## Benefits of low-dimensionality
Low-dimensionality can also be beneficial, for example when it provides robustness to noise and other sources of variability. Neuroscientists have long known the importance of so-called attractor dynamics: A [Hopfield network](https://doi.org/10.1073/pnas.79.8.2554) converges to a previously imprinted attractor state, starting from an initial state that is similar to the attractor. You can think of this as an associative memory: Only seeing part of someone's face immediately brings to mind their entire face, and even more features such as their voice. The network therefore is robust to missing or noisy information. Attractor dynamics can also make the formation of a decision robust to irrelevant information, as shown by [Finkelstein et al.](https://doi.org/10.1038/s41593-021-00840-6). Recently, [Beiran et al.](https://doi.org/10.1101/2021.11.08.467806) showed that low-dimensional neural activity can even improve generalization -- an impressive form of robustness to a mismatch between training and testing data. 

Attractor dynamics are also thought to increase developmental robustness. For example, cells adopt one out of many possible cell types, characterised by a reliable set of properties and functions. This type of ``canalisation'' makes development robust to genetic and environmental variability ([Waddington](https://doi.org/10.1038/150563a0), [Flatt](https://doi.org/10.1086/432265)). This idea is supported by the aforementioned work on the one-dimensional *Drosophila* wings ([Alba et al.](https://doi.org/10.7554/eLife.66750)). Genetic and environmental perturbations led to variation along the one principal direction, suggesting their effect is canalised by development. 

Attractor dynamics can therefore steer neural activity or development towards one of a discrete number of states, and away from competing states. This decreases the dimensionality, and that's a good thing, since it makes allows neural computation or development to resist disturbing influences. 


<img src="/images/7attractors.jpg" style="background:none; border:none; box-shadow:none;">
<span class="caption"> (a) Attractor dynamics can increase the robustness of neural activity during decision making. The black curve represents the energy landscape, the ball represents neural activity following the curve's gradient until it ends up in an attractor state.
(b) Attractor dynamics can similarly increase the robustness of developmental processes to genetic and environmental variability. This is visualize here in [Waddington's epigenetic landscape](). The ball represents a developing cell, which becomes progressively committed to a particular identity. 
(a) From [Finkelstein et al.](https://doi.org/10.1038/s41593-021-00840-6), (b) from [Waddington 1957](https://doi.org/10.4324/9781315765471)</span>

### High-dimensionality
So far, we’ve emphasised low-dimensionality. But it’s important to realise that neural activity can also be relatively high-dimensional. For example because the brain receives  high-dimensional sensory input, as when animals are watching naturalistic images. Using large scale neural recordings and sophisticated maths, [Stringer et al.](https://doi.org/10.1038/s41586-019-1346-5) showed that the dimensionality of V1 activity is, in fact, as high as possible given smoothness requirements. Intuitively, extremely high-dimensional activity requires that neurons are sensitive to minute details of the stimulus. Even the slightest change to the stimulus would then completely change neural activity, making the neural code fragile. The need for robustness therefore limits the dimensionality of neural representatins. But the representations also shouldn't be too low-dimensional, since this would make them insensitive to visual details. The key finding from Stringer et al., then, was that neural dimensionality seems to strike a near-optimal balance between robustness and efficiency. A follow up paper showed that imposing a similar structure on artificial neural networks can make them more robust to adversarial examples ([Nassar et al. 2021](https://arxiv.org/abs/2012.04729)). This is, I think, one of the more direct ways in which neuroscience has recently contributed to machine learning. 

Finally, experimental convenience can make it tempting to focus on high-variance dimensions. But the fact that other dimension capture only a small percentage of the total variance doesn't mean we can simply ignore them. For example, the lab of Mark Churchland has shown that the motor cortical dimensions predictive of muscle activity actually explain relatively little variance ([Russo et al.](https://doi.org/10.1016/j.neuron.2018.01.004)). It might therefore be better to assess a dimension's relevance based on its trial-to-trial reliability, rather than the variance it explains ([Stringer et al.](https://doi.org/10.1038/s41586-019-1346-5), [Marshall et al.](https://doi.org/10.1101/2021.05.05.442653)). 

### Takeaways: What can neuroscience learn from evolution?
The analogy between neuroscience and comparative biology suggests several lessons. First, neuroscientists, especially of the computational breed,  often refer to evolution and development as yet another type of optimisation process. This is convenient, because it justifies the use of optimisation-based models such as deep networks. But here we have seen that animals and their brains are not just shaped by adaptive processes, they're also determined by constraints. We therefore cannot assume that each experimental observation has an adaptive explanation, this has to be tested. Such a test might require causal experiments, as we saw in the butterfly example.
A second lesson is that identifying a role for neural constraints is only the first step. The second step is to understand the mechanism causing the constraint. This was possible in the butterfly, because it is a relatively simple and well-studied model organism. Determining mechanisms of neural constraints might also require a model organism that is more experimentally accessible than, for example, a primate. 
A final takeaway is that the analogous phenomena of low-d patterns in neural space and organismal/phenotypic space can interact, since the brain's structure (phenotypic space) influences its activity (neural space). A recent preprint, for example, reports a correlation across GABAergic interneurons, between the first principal component of genetic expression patterns, and spontaneous activity (Bugeon et al.). It would interesting to see if genetic perturbations induce variation along the first PC, or in different directions. It would be even more interesting, but also more challenging, to determine how much the variation along the first PC is due to selective pressure, or to constraints.

### Acknowledgements

Thanks to colleague, friend, and Twitter celebrity [Rob Lange](https://roberttlange.github.io/) for feedback on this post and encouragements to start a blog in the first place. 
