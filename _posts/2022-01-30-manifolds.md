---
image: /images/1analogy.jpg
title: "Low-dimensional manifolds in neuroscience and evolution"
---

## Low-dimensional manifolds in neuroscience and evolution

The brain contains billions of neurons, so in theory we would need a billion numbers to describe the brain's activity. But in practice, we don’t need a billion numbers -- just a handful often goes a long way. These numbers are said to define a low-dimensional *manifold* in neural space, and their existence implies that the brain typically visits only a small fraction of all its potential states. There is an analogous observation in evolutionary biology: The number of existing organisms is only a tiny fraction of all potential organisms. For example, there are no (and have never been) unicorns or centaurs, even though they are obviously conceivable to the human mind. In this blog pots, we will examine these analogous neural and evolutionary/developmental phenomena, and see what lessons can be draw from their similarity. 

<img src="/images/1analogy.jpg" style="background:none; border:none; box-shadow:none;">
<span class="caption" STYLE="font-size:80%">  (a) Population activity lies on a low-dimensional manifold in neural space. Each dimension corresponds to the activity of one neuron. (b) Variation of wing structure across a population of *Drosophila* lies on a low-dimensional manifold in phenotype space. Each dimension corresponds to the intensity of one pixel from an image of a wing. (a) From [Gallego et al. 2017](https://doi.org/10.1016/j.neuron.2017.05.025), (b) from [Alba et al. 2021](https://doi.org/10.7554/eLife.66750). </span>

Here's the table of contents:

1. TOC
{:toc}


### Low-dimensional manifolds everywhere
The activity of a large number of neurons can often be captured by a much smaller number of key dimensions. In auditory cortex, for example, neural population activity rotates in a low-dimensional space that depends on the sound stimulus, which sets the population's initial state ([Bondanelli et al.](https://doi.org/10.7554/eLife.53151)). The initial state can also depend on a subject's prior expectations, integrating them with sensory evidence in a Bayes-optimal way ([Sohn et al.](https://doi.org/10.1016/j.neuron.2019.06.012)). Although in both of these cases, neural activity is not fully explained by two or three dimensions, these dimensions do relate to behavior in an interpretable way[^1]. But human interpretability is not one of the brain’s design principles, raising the question why neural activity should be low-dimensional. 

[^1]: Interpretability is perhaps the key reason why low-dimensionality matters. Or as Geoffrey Hinton said:``To deal with a 14-dimensional space, visualize a 3-D space and say 'fourteen' to yourself very loudly. Everyone does it.''

<img src="/images/2neuralexamples.jpg" style="background:none; border:none; box-shadow:none;">
<span class="caption" STYLE="font-size:80%"> (a) Two-dimensional rotations in auditory cortex after a sound is turned off. The two colours correspond to different sounds that elicited activity in orthogonal planes. (b) Activity in frontal cortex during the estimation of the time interval between 'Set' and 'Go' cues. The activity's curvature depending on subjects' prior expectation of the time interval (short prior: red, long prior: blue), and the actual duration of the time interval (short: dark, long: light). (a) From [Bondanelli et al. 2021](https://doi.org/10.7554/eLife.53151),  (b) from [Sohn et al. 2019](https://doi.org/10.1016/j.neuron.2019.06.012).  </span>


It's much easier to observe and compare a lot of animals than to record from a lot of neurons, and that's probably why comparative biologists have long known that phenotypes are also low-dimensional. For example, most mammals have seven neck vertebrae, even though the long-necked giraffe might benefit from more, and the neckless whale could do with less. In phenotype space, therefore, only the subspace of mammals with seven neck vertebrae is densely occupied. Within-species phenotypes can also be surprisingly low-dimensional: Fruit fly wings are well-approximated by a single dimension! (See figure at the top from [Alba et al.](https://doi.org/10.7554/eLife.66750).) What causes such a sparse occupation of neural an phenotype spaces?

### Explanation 1: function 
The first explanation for low-dimensional patterns in neural and phenotype space is functional. Low-dimensional neural activity can result from low-dimensional neural computation: If a brain area is engaged in a simple computation (say, integrating a noisy variable over time), it’s activity will also be simple. This intuition was formalised by [Gao et al.](https://doi.org/10.1101/214262), who showed that the number of task variables and the smoothness of neural activity also necessarily limit the dimensionality of neural activity. The authors therefore concluded that (1) Recording only a relatively small number of neurons compared to the full population is no problem, as long as the behavior is simple enough, but (2) more complex behaviours are necessary to fully exploit large-scale recordings.
Since then, these ideas become commonplace in systems neuroscience while the field has also come to the realisation that behavior can involve much more than the experimental task at hand ([Musall et al.](https://doi.org/10.1038/s41593-019-0502-4), [Stringer et al.](https://doi.org/10.1126/science.aav7893)).

<img src="/images/3theory.jpg" style="background:none; border:none; box-shadow:none;">
<span class="caption" STYLE="font-size:80%"> If a subject is doing a relatively simple task like reaching, its neural activity varies smoothly in time, and we don't record for very long - activity will be low-dimensional. From [Gao et al. 2017](https://doi.org/10.1101/214262). </span>

Just like the brain learns to generate activity that’s suitable for the current task, natural selection favours variants that are fit to the current environment. If, therefore, only a few variants are the fittest, these are likely to spread through the population. In a famous example, black-coloured peppered moths used to be rare because they were easily spotted by predators when sitting on a light tree [Cook](https://doi.org/10.1086/378925). During the industrial revolution, however, pollution caused trees to be darker, making light-coloured moths the easier ones to spot! Natural selection therefore favoured black moths over their lighter brethren, making them the prevalent variant. This pattern reversed once pollution decreased. At any given moment, selection therefore favours a limited set of phenotypes, making alternatives rare or non-existent. Both neural activity and phenotypes can therefore be limited to a small set of “optimal” patterns. 

<img src="/images/4moths.jpg" style="background:none; border:none; box-shadow:none;">
<span class="caption" STYLE="font-size:80%">(a) Peppered moths can have a light or a dark (melanic) colour. Air pollution made trees darker, and therefore decreased the visibility of melanic moths - increasing their fitness. (b) The pattern reversed when air pollution decreased. (a) from [Wikipedia](https://commons.wikimedia.org/wiki/File:Lichte_en_zwarte_versie_berkenspanner.jpg), (b) from [Hedrick 2006](https://www.annualreviews.org/doi/abs/10.1146/annurev.ecolsys.37.091305.110132).  </span>

### Explanation 2: constraints 
But sometimes the brain cannot produce a certain pattern of neural activity, even when it would be best to do so. Similarly, a certain phenotype might fail to evolve, even when it would increase a species’ fitness or save it from extinction. In this case, the lack of variability must be due to the presence of constraints. For neural activity, such a constraint can be caused by neural connectivity. If, for example, two excitatory neurons are reciprocally connected, this might preclude activity patterns in which only one of them is active. Synaptic plasticity could weaken their connectivity, but this takes time. In the case of evolution, a constraint might be due to a single gene that affects multiple traits [pleiotropy](https://en.wikipedia.org/wiki/Pleiotropy), such that changing one trait but not the other is genetically impossible. A similar constraint can also arise when two traits are coded for by different genes, but these genes are inherited together (for example because they’re located nearby on the same chromosome). Over time, these constraints could be alleviated by mutations, but like synaptic plasticity, this takes time.

In sum, there are analogous reasons for the low-dimensional patterns in neural space and animal space. This points to fundamental similarities between their origins. Neural activity is shaped by learning, and animal forms are shaped by natural selection. Both learning and natural selection tend to increase some kind of performance or fitness measure. But both are constrained in what they can achieve, especially in the short run. 

### Probing neural manifolds
Given that there are multiple possible causes of low-d patterns, we would like to know which one is at play in any particular case. Neuroscientists and evolutionary biologists have both come up with elegant experiments to test exactly this. In neuroscience, [Sadtler et al.](https://doi.org/10.1038/nature13665) used a brain computer interface to test the neural constraints on learning. They started out by identifying a low-dimensional “intuitive manifold” of activity patterns that subjects could easily generate. The manifold was simply a principal subspace of subjects' neural activity while they passively watched the cursor moving on the screen[^2]. Next, the authors challenged their subjects to generate new patterns by perturbing the BCI mapping. Crucially, these patterns came in two types: one type was inside the intuitive manifold, the other wasn’t. The **inside manifold perturbation** still projected neural activity onto the intuitive manifold, but then permuted the manifold dimensions before feeding them to the BCI readout. Adapting to the inside-manifold perturbation therefore required subjects to change the neural activity for moving the cursor in a particular direction, but this neural activity was still part of their previous "repertoire". The **outside manifold perturbation**, on the other hand, projected activity onto a subspace different from the intuitive manifold. Controlling the cursor after this change to the BCI mapping therefore required subjects to generate patterns outside of the intuitive manifold. The key finding froom the experiment: Subjects could quickly adapt to inside manifold perturbations, but not outside manifold perturbations. This failure suggests that the low-dimensionality of the intuitive manifold was not due to functional requirements, but rather to constraints. 

[^2]: The actual procedure for training the subjects and calibrating the BCI was a bit more complicated (see Methods from [Sadtler et al.](https://doi.org/10.1038/nature13665)). If you think training a deep network is hard, try training a non-human primate! 

<img src="/images/5neuralconstraints.jpg" style="background:none; border:none; box-shadow:none;">
<span class="caption" STYLE="font-size:80%"> Neural constraints on learning [Sadtler et al. 2014](https://doi.org/10.1038/nature13665). Neural activity is usually confined to an intuitive manifold. Changes to the BCI readout within this manifold (red) are easy to learn, changes outside
  the manifold (blue) are hard to learn. </span>

### Probing evolutionary manifolds
Evolutionary-developmental biologists have similarly tested the causes of low-d phenotypes. Brakefeld et al. ([Beldade et al.](https://doi.org/10.1038/416844a), [Allen et al. 2008](https://doi.org/10.1186/1471-2148-8-94)) asked why the eyespots on the wings of a butterfly typically have are similar colour and size. Do similar eyespots convey a fitness advantage (e.g., asymmetric butterflies are picked out by predators), or are they just easier to generate by development? Brakefeld et al. put this to the test using artificial selection experiments. In one experiment, they selected butterfly strains for one of the four possible combinations of spot *size*: the two correlated combinations (small/small and large/large) and the two anti-correlated combinations (small/large and large/small). In another experiment, they selected butterflies based on the eyespot *colour*. Interestingly, the two experiments showed opposite outcomes: It was possible to evolve anti-correlated sizes of spots, but not to evolve anti-correlated colours. This suggests that similarly sized spots are the result of selection (different sizes are possible, but apparently not selected), but similarly coloured spots colours are (also) the result of constraints (different colours are not possible).

<img src="/images/6devoconstraints.jpg" style="background:none; border:none; box-shadow:none;">
<span class="caption" STYLE="font-size:80%"> Developmental constraints on learning ([Allen et al. 2008](https://doi.org/10.1186/1471-2148-8-94)). The eye spots on butterfly wings are correlated in both size and colour. Artificial selection cannot break the correlation between the colours (left), but it can break the correlation between the sizes (right). </span>

The difference in flexibility suggests that the colour and size of eyespots are determined by different developmental mechanisms. This is indeed the case. 
During development, eyespot size is regulated by signalling from clusters of ''organiser'' cells. Each eyespot has its own organiser, and its mature size will therefore be relatively independent of neighbouring eyespots. Eyespot colour, on the other hand, develops in response to the concentration gradient of a diffusive signal. This concentration cannot be regulated independently for each eyespot, and neither can the response to a given concentration.  

The different causes of phenotypic correlations would have been difficult to predict from observational data alone, since the correlation across eyespot color and size are similar in wild type butterflies. These experiments therefore not only show that either selective pressure or genetic constraints might can a role, they also show that disambiguating the two requires causal experiments. 


### Benefits of low-dimensionality
Low-dimensionality can also be beneficial, for example when it provides robustness to noise and other sources of variability. Neuroscientists have long known the importance of so-called attractor dynamics: A [Hopfield network](https://doi.org/10.1073/pnas.79.8.2554) converges to a previously imprinted attractor state, starting from an initial state that is similar to the attractor. The network therefore is robust to missing or noisy information. Attractor dynamics can also make the formation of a decision robust to irrelevant information, as shown by [Finkelstein et al.](https://doi.org/10.1038/s41593-021-00840-6). 

Attractor dynamics are also thought to increase developmental robustness. For example, cells adopt one out of many possible cell types, characterised by a reliable set of properties and functions. This type of ``canalisation'' makes development robust to genetic and environmental variability ([Waddington](https://doi.org/10.1038/150563a0), [Flatt](https://doi.org/10.1086/432265)). This idea is supported by the aforementioned work on the one-dimensional *Drosophila* wings ([Alba et al.](https://doi.org/10.7554/eLife.66750)). Genetic and environmental perturbations led to variation along the one principal direction, suggesting their effect is canalised by development. 

Attractor dynamics can therefore steer neural activity or development towards one of a discrete number of states, and away from competing states. This decreases dimensionality, and that's a good thing, since it makes allows neural computation or development to resist disturbing influences. 

<img src="/images/7attractors.jpg" style="background:none; border:none; box-shadow:none;">
<span class="caption" STYLE="font-size:80%"> (a) Attractor dynamics can increase the robustness of neural activity during decision making. The black curve represents the energy landscape, the ball represents neural activity following the curve's gradient until it ends up in an attractor state.
(b) Attractor dynamics can similarly increase the robustness of developmental processes to genetic and environmental variability. This is visualize here in [Waddington's epigenetic landscape](). The ball represents a developing cell, which becomes progressively committed to a particular identity. 
(a) From [Finkelstein et al.](https://doi.org/10.1038/s41593-021-00840-6), (b) from [Waddington 1957](https://doi.org/10.4324/9781315765471) </span>

### Counterpoint: What about high-dimensionality?
So far, we’ve emphasised low-dimensionality, but neural activity can also be high-dimensional. For example because the brain receives complex sensory input when animals are watching naturalistic images. Using large scale neural recordings and sophisticated maths, [Stringer et al.](https://doi.org/10.1038/s41586-019-1346-5) showed that the dimensionality of V1 activity is, in fact, as high as possible given smoothness requirements. Intuitively, extremely high-dimensional activity requires that neurons are sensitive to minute details of the stimulus. Even the slightest change would then completely change neural activity, making the neural code fragile. The need for robustness therefore limits the dimensionality of neural representations. But neural representations also shouldn't be too low-dimensional, since this would make them insensitive to visual details. The key finding from Stringer et al., then, was that neural dimensionality seems to strike a near-optimal balance between robustness and efficiency. A follow-up paper showed that imposing a similar structure on artificial neural networks can make them more robust to adversarial examples ([Nassar et al.](https://arxiv.org/abs/2012.04729)). This is, I think, one of the more direct ways in which neuroscience has recently contributed to machine learning. 

Finally, experimental convenience can make it tempting to focus on high-variance dimensions. But the fact that other dimension capture only a small percentage of the total variance doesn't mean we can simply ignore them. For example, the lab of Mark Churchland has shown that the motor cortical dimensions predictive of muscle activity actually explain relatively little variance ([Russo et al.](https://doi.org/10.1016/j.neuron.2018.01.004)), even though they clearly contribute to behaviour! It might therefore be better to assess a dimension's relevance based on its trial-to-trial reliability, rather than the variance it explains ([Stringer et al.](https://doi.org/10.1038/s41586-019-1346-5), [Marshall et al.](https://doi.org/10.1101/2021.05.05.442653)). 

### Takeaways for neuroscience
The analogy between neuroscience and comparative biology suggests several lessons. First, computational neuroscientists often refer to evolution and development as yet another type of optimisation process. But here we have seen that animals and their brains are not just shaped by adaptive processes, they're also determined by constraints. We therefore cannot assume that each experimental observation has an adaptive explanation, this has to be tested[^3]. Such a test might require causal experiments, as we saw in the butterfly example.  

[^3]: I'm by no means the first one to make this point, see for example the classic [Spandrels paper](https://doi.org/10.1098/rspb.1979.0086) from Gould & Lewontin. 

A second lesson is that identifying a role for neural constraints is only the first step. The second step is to understand the mechanism causing the constraint. This was possible in the butterfly, because it is a relatively simple and well-studied model organism. Determining mechanisms of neural constraints might also require a model organism that is more experimentally accessible than, for example, a primate. 

A final takeaway is that the analogous phenomena of low-d patterns in neural space and organismal/phenotypic space interact. The brain's structure (phenotypic space) influences its activity (neural space). Conversely, the brain's activity can influence its evolution by shaping behavior. 


### Acknowledgements

Thanks to [Rob Lange](https://roberttlange.github.io/) for feedback on this post and encouragement to start blogging in the first place. 
